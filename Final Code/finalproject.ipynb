{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d06237df",
   "metadata": {},
   "source": [
    "### **Data Gathering**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0de16f90",
   "metadata": {},
   "source": [
    "Housing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6697d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import http.client, urllib.parse\n",
    "import csv\n",
    "\n",
    "# HOUSING API\n",
    "def getHouses(csv_, cities):\n",
    "\n",
    "    csvfile = open(csv_, 'w', newline='')\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=['address', 'price', 'city', 'squareFootage', 'latitude', 'longitude'])\n",
    "\n",
    "    url = \"https://realty-mole-property-api.p.rapidapi.com/saleListings\"\n",
    "\n",
    "    for city, state in cities:\n",
    "        querystring = {\"city\":city,\"state\":state,\"limit\":\"500\"}\n",
    "\n",
    "        headers = {\n",
    "            \"X-RapidAPI-Key\": \"43686bd243mshe8a0be6f9e0556cp10a1bbjsn58033bf0f546\",\n",
    "            \"X-RapidAPI-Host\": \"realty-mole-property-api.p.rapidapi.com\"\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "        for house in response.json():\n",
    "            if type(house) == dict:\n",
    "                address = house.get('formattedAddress')\n",
    "                price = house.get('price')\n",
    "                city = house.get('city')\n",
    "                sqf = house.get('squareFootage')\n",
    "                latitude = house.get('latitude')\n",
    "                longitude = house.get('longitude')\n",
    "                writer.writerow({'address':address, 'price':price, 'city':city, 'squareFootage':sqf, \n",
    "                                 'latitude':latitude, 'longitude':longitude})\n",
    "\n",
    "    csvfile.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUNNING THIS WILL DUPLICATE ENTRIES ON CURRENT FILE\n",
    "cities = [('Los Angeles', 'CA'), ('Anaheim','CA'), ('Long Beach', 'CA'), ('Chicago', 'IL'), ('Naperville', 'IL'), ('Elgin', 'IL'),\n",
    "             ('Dallas', 'TX'), ('Fort Worth', 'TX'), ('Arlington', 'TX'), ('Washington', 'DC'), ('Arlington', 'VA'), ('Alexandria', 'VA')]\n",
    "\n",
    "getHouses('houses.csv', cities)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "774bdfa7",
   "metadata": {},
   "source": [
    "Yellow Pages API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d143de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import lxml.html as lx\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da549121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPlaces(city, state, term):\n",
    "    addresses = list()\n",
    "    for i in range(1, 6):\n",
    "        if i == 1:\n",
    "            url = \"https://www.yellowpages.com/search?search_terms=\" + str(term) + \"&geo_location_terms=\" + str(city) + \"%2C+\" + str(state)\n",
    "        else:\n",
    "            url = \"https://www.yellowpages.com/search?search_terms=\" + str(term) + \"&geo_location_terms=\" + str(city) + \"%2C+\" + str(state) + \"&page=\" + str(i)\n",
    "        time.sleep(0.05)\n",
    "        response = requests.get(url,\n",
    "                            headers = {\"accept\" : \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "                                        \"accept-encoding\": \"gzip, deflate, br\",\n",
    "                                        \"accept-language\": \"en-US,en;q=0.9\",\n",
    "                                        \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"})\n",
    "\n",
    "        html = response.text\n",
    "        page = BeautifulSoup(html, \"html.parser\")\n",
    "        address = str(page.find_all(\"div\", class_=\"adr\")).split('<div class=\"adr\"><div class=\"street-address\">') \n",
    "\n",
    "        for j in range(1, len(address)):\n",
    "            if address[j] not in addresses:\n",
    "                addresses.append(address[j].replace('</div><div class=\"locality\">', \",\").replace('</div></div>, ', \"\").replace(\"</div></div>]\", \"\").replace('<div class=\"adr\"><div class=\"locality\">', \"\"))\n",
    "            else:\n",
    "                continue\n",
    "    return pd.DataFrame(pd.Series(addresses).unique(), columns = [\"Address\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "places = ['hospital', 'grocery', \"gym\", \"parks\", \"beaches\", \"cemetary\", \"shopping\", \"resturant\", \"golf\"]\n",
    "\n",
    "LosAngeles = pd.DataFrame()\n",
    "Anaheim = pd.DataFrame()\n",
    "LongBeach = pd.DataFrame()\n",
    "Chicago = pd.DataFrame()\n",
    "Naperville = pd.DataFrame()\n",
    "Elgin = pd.DataFrame()\n",
    "Dallas = pd.DataFrame()\n",
    "FortWorth = pd.DataFrame()\n",
    "ArlingtonTX = pd.DataFrame()\n",
    "Washington = pd.DataFrame()\n",
    "ArlingtonVA = pd.DataFrame()\n",
    "Alexandria = pd.DataFrame()\n",
    "\n",
    "for p in range(0, len(places)):\n",
    "    time.sleep(0.05)\n",
    "    name = places[p]\n",
    "    LosAngeles[name] = getPlaces('Los+Angeles', 'CA', places[p])\n",
    "    Anaheim[name] = getPlaces('Anaheim', 'CA', places[p])\n",
    "    LongBeach[name] = getPlaces('Long+Beach', 'CA', places[p])\n",
    "    Chicago[name] = getPlaces('Chicago', 'IL', places[p])\n",
    "    Naperville[name] = getPlaces('Naperville', 'IL', places[p])\n",
    "    Elgin[name] = getPlaces('Elgin', 'IL', places[p])\n",
    "    Dallas[name] = getPlaces('Dallas', 'TX', places[p])\n",
    "    FortWorth[name] = getPlaces('Fort+Worth', 'TX', places[p])\n",
    "    ArlingtonTX[name] = getPlaces('Arlington', 'TX', places[p])\n",
    "    Washington[name] = getPlaces('Washington', 'DC', places[p])\n",
    "    ArlingtonVA[name] = getPlaces('Arlington', 'VA', places[p])\n",
    "    Alexandria[name] = getPlaces('Alexandria', 'VA', places[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "thingsToDo = pd.concat([LosAngeles, Anaheim, LongBeach, \n",
    "                   Chicago, Naperville, Elgin, \n",
    "                   Dallas,FortWorth, ArlingtonTX, \n",
    "                   Washington, ArlingtonVA, Alexandria])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "888ee2ff",
   "metadata": {},
   "source": [
    "Common Core Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f68c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn data into address\n",
    "def address(x):\n",
    "    x = x.iloc[:,range(9,13)]\n",
    "    x['address'] = x[x.columns[:]].apply(\n",
    "        lambda x: ', '.join(x.dropna().astype(str)),\n",
    "        axis=1\n",
    "    )\n",
    "    x = pd.DataFrame(x[x.columns[4]])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5341243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data in\n",
    "#La\n",
    "la = pd.read_excel(\"./data/la.xlsx\")\n",
    "la = address(la)\n",
    "long_beach = pd.read_excel(\"./data/long_beach.xlsx\")\n",
    "long_beach = address(long_beach)\n",
    "anaheim = pd.read_excel(\"./data/anaheim.xlsx\")\n",
    "anaheim = address(anaheim)\n",
    "\n",
    "#Chicago\n",
    "chicago = pd.read_excel(\"./data/chicago.xlsx\")\n",
    "chicago = address(chicago)\n",
    "naperville = pd.read_excel(\"./data/naperville.xlsx\")\n",
    "naperville = address(naperville)\n",
    "elgin = pd.read_excel(\"./data/elgin.xlsx\")\n",
    "elgin = address(elgin)\n",
    "\n",
    "#Dallas\n",
    "dallas = pd.read_excel(\"./data/dallas.xlsx\")\n",
    "dallas = address(dallas)\n",
    "fort_worth = pd.read_excel(\"./data/fort_worth.xlsx\")\n",
    "fort_worth = address(fort_worth)\n",
    "arlington = pd.read_excel(\"./data/arlington.xlsx\")\n",
    "arlington = address(arlington)\n",
    "\n",
    "#Washington\n",
    "washington = pd.read_excel(\"./data/dc.xlsx\")\n",
    "washington = address(washington)\n",
    "arlington_va = pd.read_excel(\"./data/arlington_va.xlsx\")\n",
    "arlington_va = address(arlington_va)\n",
    "alexandria = pd.read_excel(\"./data/alexandria.xlsx\")\n",
    "alexandria = address(alexandria)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d06237df",
   "metadata": {},
   "source": [
    "### **Data Manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2714c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import http.client, urllib.parse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "from geopy.distance import geodesic\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "377c5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDist(houses, locations):\n",
    "    distances = []\n",
    "        \n",
    "    for ind in houses.index:\n",
    "        y1 = houses['Longitude'][ind]\n",
    "        x1 = houses['Latitude'][ind]\n",
    "        coord1 = (y1,x1)\n",
    "        min_dist = 9999999\n",
    "        for i in locations.index:\n",
    "            y2 = locations['Latitude'][i]\n",
    "            x2 = locations['Longitude'][i]\n",
    "            coord2 = (y2,x2)\n",
    "            try:\n",
    "                dist = geodesic(coord1, coord2).miles\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "            except:\n",
    "                pass\n",
    "        distances.append(min_dist)  \n",
    "\n",
    "        \n",
    "    return distances\n",
    "\n",
    "def getCoords(df):\n",
    "    \n",
    "    #api info\n",
    "    conn = http.client.HTTPConnection('api.positionstack.com')\n",
    "    key = 'df3b33e27a0f6451fd9aae993c6a26fa'\n",
    "    \n",
    "    \n",
    "    def query(coord, address):\n",
    "\n",
    "        params = urllib.parse.urlencode({\n",
    "            'access_key': key,\n",
    "            'query': address,\n",
    "            'limit': 1\n",
    "            })\n",
    "        \n",
    "        conn.request('GET', '/v1/forward?{}'.format(params))\n",
    "        results = conn.getresponse()\n",
    "        data = json.loads(results.read())['data'][0] #converts json to dict\n",
    "        \n",
    "        if coord == 'x':\n",
    "            result = data['latitude']\n",
    "        else:\n",
    "            result = data['longitude']\n",
    "            \n",
    "            return result\n",
    "    \n",
    "\n",
    "    lat = [query('x', address) for address in df.iloc[:,0]]\n",
    "    lon = [query('y', address) for address in df.iloc[:,0]]\n",
    "    \n",
    "    df['latitude'] = lat\n",
    "    df['longitude'] = lon\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6248b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT AND ORGANIZE DATA\n",
    "\n",
    "houses = pd.read_csv('../data/final/houses_clean.csv')\n",
    "\n",
    "houses_la = houses[houses['City'] == 'Los Angeles']\n",
    "houses_anaheim = houses[houses['City'] == 'Anaheim']\n",
    "houses_longbeach = houses[houses['City'] == 'Long Beach']\n",
    "\n",
    "houses_dallas = houses[houses['City'] == 'Dallas']\n",
    "houses_fortworth = houses[houses['City'] == 'Fort Worth']\n",
    "houses_arlington_tx = houses[houses['City'] == 'ArlingtonTX']\n",
    "\n",
    "houses_chicago = houses[houses['City'] == 'Chicago']\n",
    "houses_naperville = houses[houses['City'] == 'Naperville']\n",
    "houses_elgin = houses[houses['City'] == 'Elgin']\n",
    "\n",
    "houses_washington = houses[houses['City'] == 'Washington']\n",
    "houses_alexandria = houses[houses['City'] == 'Alexandria']\n",
    "houses_arlington_va = houses[houses['City'] == 'ArlingtonVA']\n",
    "\n",
    "\n",
    "schools = pd.read_csv('../data/final/schools.csv')\n",
    "schools_la = schools[schools['City'] == 'Los Angeles']\n",
    "schools_anaheim = schools[schools['City'] == 'Anaheim']\n",
    "schools_longbeach = schools[schools['City'] == 'Long Beach']\n",
    "\n",
    "schools_dallas = schools[schools['City'] == 'Dallas']\n",
    "schools_fortworth = schools[schools['City'] == 'Fort Worth']\n",
    "schools_atx = schools[schools['City'] == 'ArlingtonTX']\n",
    "\n",
    "schools_chicago = schools[schools['City'] == 'Chicago']\n",
    "schools_naperville = schools[schools['City'] == 'Naperville']\n",
    "schools_elgin = schools[schools['City'] == 'Elgin']\n",
    "\n",
    "schools_washington = schools[schools['City'] == 'Washington']\n",
    "schools_alex = schools[schools['City'] == 'Alexandria']\n",
    "schools_ava = schools[schools['City'] == 'ArlingtonVA']\n",
    "\n",
    "\n",
    "hospitals = pd.read_csv('../data/final/hospital.csv')\n",
    "\n",
    "hospitals_la = hospitals[hospitals['City'] == 'Los Angeles']\n",
    "hospitals_a = hospitals[hospitals['City'] == 'Anaheim']\n",
    "hospitals_lb = hospitals[hospitals['City'] == 'Long Beach']\n",
    "\n",
    "hospitals_d = hospitals[hospitals['City'] == 'Dallas']\n",
    "hospitals_fw = hospitals[hospitals['City'] == 'Fort Worth']\n",
    "hospitals_atx = hospitals[hospitals['City'] == 'ArlingtonTX']\n",
    "\n",
    "hospitals_c = hospitals[hospitals['City'] == 'Chicago']\n",
    "hospitals_n = hospitals[hospitals['City'] == 'Naperville']\n",
    "hospitals_e = hospitals[hospitals['City'] == 'Elgin']\n",
    "\n",
    "hospitals_dc = schools[schools['City'] == 'Washington']\n",
    "hospitals_alex = schools[schools['City'] == 'Alexandria']\n",
    "hospitals_ava = schools[schools['City'] == 'ArlingtonVA']\n",
    "\n",
    "grocery = pd.read_csv('../data/final/grocery.csv')\n",
    "\n",
    "grocery_la = grocery[grocery['City'] == 'Los Angeles']\n",
    "grocery_a = grocery[grocery['City'] == 'Anaheim']\n",
    "grocery_lb = grocery[grocery['City'] == 'Long Beach']\n",
    "\n",
    "grocery_d = grocery[grocery['City'] == 'Dallas']\n",
    "grocery_fw = grocery[grocery['City'] == 'Fort Worth']\n",
    "grocery_atx = grocery[grocery['City'] == 'ArlingtonTX']\n",
    "\n",
    "grocery_c = grocery[grocery['City'] == 'Chicago']\n",
    "grocery_n = grocery[grocery['City'] == 'Naperville']\n",
    "grocery_e = grocery[grocery['City'] == 'Elgin']\n",
    "\n",
    "grocery_dc = grocery[grocery['City'] == 'Washington']\n",
    "grocery_alex = grocery[grocery['City'] == 'Alexandria']\n",
    "grocery_ava = grocery[grocery['City'] == 'ArlingtonVA']\n",
    "\n",
    "gym = pd.read_csv('../data/final/gym.csv')\n",
    "\n",
    "gym_la = gym[gym['City'] == 'Los Angeles']\n",
    "gym_lb = gym[gym['City'] == 'Long Beach']\n",
    "gym_a = gym[gym['City'] == 'Anaheim']\n",
    "\n",
    "gym_d = gym[gym['City'] == 'Dallas']\n",
    "gym_atx = gym[gym['City'] == 'ArlingtonTX']\n",
    "gym_fw = gym[gym['City'] == 'Fort Worth']\n",
    "\n",
    "gym_c = gym[gym['City'] == 'Chicago']\n",
    "gym_e = gym[gym['City'] == 'Elgin']\n",
    "gym_n = gym[gym['City'] == 'Naperville']\n",
    "\n",
    "gym_dc = gym[gym['City'] == 'Washington']\n",
    "gym_ava = gym[gym['City'] == 'ArlingtonVA']\n",
    "gym_alex = gym[gym['City'] == 'Alexandria']\n",
    "\n",
    "parks = pd.read_csv('../data/final/parks.csv')\n",
    "\n",
    "parks_la = parks[parks['City'] == 'Los Angeles']\n",
    "parks_lb = parks[parks['City'] == 'Long Beach']\n",
    "parks_a = parks[parks['City'] == 'Anaheim']\n",
    "\n",
    "parks_d = parks[parks['City'] == 'Dallas']\n",
    "parks_atx = parks[parks['City'] == 'ArlingtonTX']\n",
    "parks_fw = parks[parks['City'] == 'Fort Worth']\n",
    "\n",
    "parks_c = parks[parks['City'] == 'Chicago']\n",
    "parks_e = parks[parks['City'] == 'Elgin']\n",
    "parks_n = parks[parks['City'] == 'Naperville']\n",
    "\n",
    "parks_dc = parks[parks['City'] == 'Washington']\n",
    "parks_ava = parks[parks['City'] == 'ArlingtonVA']\n",
    "parks_alex = parks[parks['City'] == 'Alexandria']\n",
    "\n",
    "beaches = pd.read_csv('../data/final/beach.csv')\n",
    "\n",
    "beaches_la = beaches[beaches['City'] == 'Los Angeles']\n",
    "beaches_lb = beaches[beaches['City'] == 'Long Beach']\n",
    "beaches_a = beaches[beaches['City'] == 'Anaheim']\n",
    "\n",
    "beaches_d = beaches[beaches['City'] == 'Dallas']\n",
    "beaches_atx = beaches[beaches['City'] == 'ArlingtonTX']\n",
    "beaches_fw = beaches[beaches['City'] == 'Fort Worth']\n",
    "\n",
    "beaches_c = beaches[beaches['City'] == 'Chicago']\n",
    "beaches_e = beaches[beaches['City'] == 'Elgin']\n",
    "beaches_n = beaches[beaches['City'] == 'Naperville']\n",
    "\n",
    "beaches_dc = beaches[beaches['City'] == 'Washington']\n",
    "beaches_ava = beaches[beaches['City'] == 'ArlingtonVA']\n",
    "beaches_alex = beaches[beaches['City'] == 'Alexandria']\n",
    "\n",
    "cemetary = pd.read_csv('../data/final/cemetary.csv')\n",
    "\n",
    "cemetary_la = cemetary[cemetary['City'] == 'Los Angeles']\n",
    "cemetary_lb = cemetary[cemetary['City'] == 'Long Beach']\n",
    "cemetary_a = cemetary[cemetary['City'] == 'Anaheim']\n",
    "\n",
    "cemetary_d = cemetary[cemetary['City'] == 'Dallas']\n",
    "cemetary_atx = cemetary[cemetary['City'] == 'ArlingtonTX']\n",
    "cemetary_fw = cemetary[cemetary['City'] == 'Fort Worth']\n",
    "\n",
    "cemetary_c = cemetary[cemetary['City'] == 'Chicago']\n",
    "cemetary_e = cemetary[cemetary['City'] == 'Elgin']\n",
    "cemetary_n = cemetary[cemetary['City'] == 'Naperville']\n",
    "\n",
    "cemetary_dc = cemetary[cemetary['City'] == 'Washington']\n",
    "cemetary_ava = cemetary[cemetary['City'] == 'ArlingtonVA']\n",
    "cemetary_alex = cemetary[cemetary['City'] == 'Alexandria']\n",
    "\n",
    "shopping = pd.read_csv('../data/final/shopping.csv')\n",
    "\n",
    "shopping_la = shopping[shopping['City'] == 'Los Angeles']\n",
    "shopping_lb = shopping[shopping['City'] == 'Long Beach']\n",
    "shopping_a = shopping[shopping['City'] == 'Anaheim']\n",
    "\n",
    "shopping_d = shopping[shopping['City'] == 'Dallas']\n",
    "shopping_atx = shopping[shopping['City'] == 'ArlingtonTX']\n",
    "shopping_fw = shopping[shopping['City'] == 'Fort Worth']\n",
    "\n",
    "shopping_c = shopping[shopping['City'] == 'Chicago']\n",
    "shopping_e = shopping[shopping['City'] == 'Elgin']\n",
    "shopping_n = shopping[shopping['City'] == 'Naperville']\n",
    "\n",
    "shopping_dc = shopping[shopping['City'] == 'Washington']\n",
    "shopping_ava = shopping[shopping['City'] == 'ArlingtonVA']\n",
    "shopping_alex = shopping[shopping['City'] == 'Alexandria']\n",
    "\n",
    "resturant = pd.read_csv('../data/final/restaurants.csv')\n",
    "\n",
    "resturant_la = resturant[resturant['City'] == 'Los Angeles']\n",
    "resturant_lb = resturant[resturant['City'] == 'Long Beach']\n",
    "resturant_a = resturant[resturant['City'] == 'Anaheim']\n",
    "\n",
    "resturant_d = resturant[resturant['City'] == 'Dallas']\n",
    "resturant_atx = resturant[resturant['City'] == 'ArlingtonTX']\n",
    "resturant_fw = resturant[resturant['City'] == 'Fort Worth']\n",
    "\n",
    "resturant_c = resturant[resturant['City'] == 'Chicago']\n",
    "resturant_e = resturant[resturant['City'] == 'Elgin']\n",
    "resturant_n = resturant[resturant['City'] == 'Naperville']\n",
    "\n",
    "resturant_dc = resturant[resturant['City'] == 'Washington']\n",
    "resturant_ava = resturant[resturant['City'] == 'ArlingtonVA']\n",
    "resturant_alex = resturant[resturant['City'] == 'Alexandria']\n",
    "\n",
    "golf = pd.read_csv('../data/final/golf.csv')\n",
    "\n",
    "golf_la = golf[golf['City'] == 'Los Angeles']\n",
    "golf_lb = golf[golf['City'] == 'Long Beach']\n",
    "golf_a = golf[golf['City'] == 'Anaheim']\n",
    "\n",
    "golf_d = golf[golf['City'] == 'Dallas']\n",
    "golf_atx = golf[golf['City'] == 'ArlingtonTX']\n",
    "golf_fw = golf[golf['City'] == 'Fort Worth']\n",
    "\n",
    "golf_c = golf[golf['City'] == 'Chicago']\n",
    "golf_e = golf[golf['City'] == 'Elgin']\n",
    "golf_n = golf[golf['City'] == 'Naperville']\n",
    "\n",
    "golf_dc = golf[golf['City'] == 'Washington']\n",
    "golf_ava = golf[golf['City'] == 'ArlingtonVA']\n",
    "golf_alex = golf[golf['City'] == 'Alexandria']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a070b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "\n",
    "houses = pd.read_csv('../data/houses_clean.csv')\n",
    "\n",
    "houses_la = houses.loc[houses['City'] == 'Anaheim']\n",
    "houses_anaheim = houses.loc[houses['City'] == 'Los Angeles']\n",
    "houses_longbeach = houses.loc[houses['City'] == 'Long Beach']\n",
    "\n",
    "houses_dallas = houses.loc[houses['City'] == 'Dallas']\n",
    "houses_fortworth = houses.loc[houses['City'] == 'Fort Worth']\n",
    "houses_arlington_tx = houses.loc[houses['City'] == 'ArlingtonTX']\n",
    "\n",
    "houses_chicago = houses.loc[houses['City'] == 'Chicago']\n",
    "houses_naperville = houses.loc[houses['City'] == 'Naperville']\n",
    "houses_elgin = houses.loc[houses['City'] == 'Elgin']\n",
    "\n",
    "houses_washington = houses.loc[houses['City'] == 'Washington']\n",
    "houses_alexandria = houses.loc[houses['City'] == 'Alexandria']\n",
    "houses_arlington_va = houses.loc[houses['City'] == 'ArlingtonVA']\n",
    "\n",
    "#combine cities by state\n",
    "houses_ca = pd.concat([houses_la, houses_anaheim, houses_longbeach])\n",
    "houses_tx = pd.concat([houses_dallas, houses_fortworth, houses_arlington_tx])\n",
    "houses_dc = pd.concat([houses_washington, houses_alexandria, houses_arlington_va])\n",
    "houses_il = pd.concat([houses_chicago, houses_naperville, houses_elgin])\n",
    "\n",
    "#divide price by sq ft\n",
    "houses_ca['Housing Price/SQ Ft'] = houses_ca['Housing Price']/houses_ca['SQ Ft']\n",
    "houses_tx['Housing Price/SQ Ft'] = houses_tx['Housing Price']/houses_tx['SQ Ft']\n",
    "houses_dc['Housing Price/SQ Ft'] = houses_dc['Housing Price']/houses_dc['SQ Ft']\n",
    "houses_il['Housing Price/SQ Ft'] = houses_il['Housing Price']/houses_il['SQ Ft']\n",
    "\n",
    "#houses_ca['log'] = np.log(houses_ca['Housing Price/SQ Ft'])\n",
    "#houses_ca['log2'] = np.log2(houses_ca['Housing Price/SQ Ft'])\n",
    "#houses_ca['log10'] = np.log10(houses_ca['Housing Price/SQ Ft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb506ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_tx.sort_values(by = 'Housing Price/SQ Ft', ascending=True).head()\n",
    "houses_il.sort_values(by = 'Housing Price/SQ Ft', ascending=False).head()\n",
    "houses_ca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ee1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(houses_il['Housing Price/SQ Ft'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3f63cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(houses_il['Housing Price/SQ Ft'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf96898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize\n",
    "houses_ca['Norm Price'] = preprocessing.MinMaxScaler().fit_transform(np.array(houses_ca['Housing Price/SQ Ft']).reshape(-1,1))\n",
    "houses_tx['Norm Price'] = preprocessing.MinMaxScaler().fit_transform(np.array(houses_tx['Housing Price/SQ Ft']).reshape(-1,1))\n",
    "houses_dc['Norm Price'] = preprocessing.MinMaxScaler().fit_transform(np.array(houses_dc['Housing Price/SQ Ft']).reshape(-1,1))\n",
    "houses_il['Norm Price'] = preprocessing.MinMaxScaler().fit_transform(np.array(houses_il['Housing Price/SQ Ft']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea00131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize houses for pricing\n",
    "houses = pd.read_csv('../data/houses_clean.csv')\n",
    "\n",
    "houses_la = houses.loc[houses['City'] == 'Anaheim']\n",
    "houses_anaheim = houses.loc[houses['City'] == 'Los Angeles']\n",
    "houses_longbeach = houses.loc[houses['City'] == 'Long Beach']\n",
    "\n",
    "houses_dallas = houses.loc[houses['City'] == 'Dallas']\n",
    "houses_fortworth = houses.loc[houses['City'] == 'Fort Worth']\n",
    "houses_arlington_tx = houses.loc[houses['City'] == 'ArlingtonTX']\n",
    "\n",
    "houses_chicago = houses.loc[houses['City'] == 'Chicago']\n",
    "houses_naperville = houses.loc[houses['City'] == 'Naperville']\n",
    "houses_elgin = houses.loc[houses['City'] == 'Elgin']\n",
    "\n",
    "houses_washington = houses.loc[houses['City'] == 'Washington']\n",
    "houses_alexandria = houses.loc[houses['City'] == 'Alexandria']\n",
    "houses_arlington_va = houses.loc[houses['City'] == 'ArlingtonVA']\n",
    "\n",
    "#combine cities by state\n",
    "houses_ca = pd.concat([houses_la, houses_anaheim, houses_longbeach])\n",
    "houses_tx = pd.concat([houses_dallas, houses_fortworth, houses_arlington_tx])\n",
    "houses_dc = pd.concat([houses_washington, houses_alexandria, houses_arlington_va])\n",
    "houses_il = pd.concat([houses_chicago, houses_naperville, houses_elgin])\n",
    "\n",
    "#divide price by sq ft\n",
    "houses_ca['Housing Price/SQ Ft'] = houses_ca['Housing Price']/houses_ca['SQ Ft']\n",
    "houses_tx['Housing Price/SQ Ft'] = houses_tx['Housing Price']/houses_tx['SQ Ft']\n",
    "houses_dc['Housing Price/SQ Ft'] = houses_dc['Housing Price']/houses_dc['SQ Ft']\n",
    "houses_il['Housing Price/SQ Ft'] = houses_il['Housing Price']/houses_il['SQ Ft']\n",
    "\n",
    "#clean data\n",
    "houses_ca = houses_ca.drop(houses_ca[houses_ca['Housing Price/SQ Ft'] > 3000].index)\n",
    "houses_dc = houses_dc.drop(houses_dc[houses_dc['Housing Price/SQ Ft'] > 3000].index)\n",
    "\n",
    "#add column\n",
    "#houses_ca['Housing Price/SQ Ft'] = pd.DataFrame(houses_ca['Housing Price/SQ Ft'])\n",
    "#houses_tx['Housing Price/SQ Ft'] = pd.DataFrame(houses_tx['Housing Price/SQ Ft'])\n",
    "#houses_dc['Housing Price/SQ Ft'] = pd.DataFrame(houses_dc['Housing Price/SQ Ft'])\n",
    "#houses_il['Housing Price/SQ Ft'] = pd.DataFrame(houses_il['Housing Price/SQ Ft'])\n",
    "\n",
    "#normalize\n",
    "houses_ca['Norm Price'] = preprocessing.MinMaxScaler().fit_transform(np.array(houses_ca['Housing Price/SQ Ft']).reshape(-1,1))\n",
    "houses_tx['Norm Price'] = preprocessing.MinMaxScaler().fit_transform(np.array(houses_tx['Housing Price/SQ Ft']).reshape(-1,1))\n",
    "houses_dc['Norm Price'] = preprocessing.MinMaxScaler().fit_transform(np.array(houses_dc['Housing Price/SQ Ft']).reshape(-1,1))\n",
    "houses_il['Norm Price'] = preprocessing.MinMaxScaler().fit_transform(np.array(houses_il['Housing Price/SQ Ft']).reshape(-1,1))\n",
    "\n",
    "#subset so that all city dataframes have the normalized price values\n",
    "houses_la = houses_ca.loc[houses_ca['City'] == 'Anaheim']\n",
    "houses_anaheim = houses_ca.loc[houses_ca['City'] == 'Los Angeles']\n",
    "houses_longbeach = houses_ca.loc[houses_ca['City'] == 'Long Beach']\n",
    "\n",
    "houses_dallas = houses_tx.loc[houses_tx['City'] == 'Dallas']\n",
    "houses_fortworth = houses_tx.loc[houses_tx['City'] == 'Fort Worth']\n",
    "houses_arlington_tx = houses_tx.loc[houses_tx['City'] == 'ArlingtonTX']\n",
    "\n",
    "houses_chicago = houses_il.loc[houses_il['City'] == 'Chicago']\n",
    "houses_naperville = houses_il.loc[houses_il['City'] == 'Naperville']\n",
    "houses_elgin = houses_il.loc[houses_il['City'] == 'Elgin']\n",
    "\n",
    "houses_washington = houses_dc.loc[houses_dc['City'] == 'Washington']\n",
    "houses_alexandria = houses_dc.loc[houses_dc['City'] == 'Alexandria']\n",
    "houses_arlington_va = houses_dc.loc[houses_dc['City'] == 'ArlingtonVA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "46df46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LA DATA\n",
    "hospitals_ca = pd.concat([hospitals_la, hospitals_lb, hospitals_a])\n",
    "gym_ca = pd.concat([gym_la, gym_lb, gym_a])\n",
    "cemetary_ca = pd.concat([cemetary_la, cemetary_lb, cemetary_a])\n",
    "parks_ca = pd.concat([parks_la, parks_lb, parks_la])\n",
    "beaches_ca = pd.concat([beaches_la, beaches_lb, beaches_a])\n",
    "shopping_ca = pd.concat([shopping_la, shopping_lb, shopping_a])\n",
    "grocery_ca = pd.concat([grocery_la, grocery_lb, grocery_a])\n",
    "resturant_ca = pd.concat([resturant_la, resturant_lb, resturant_a])\n",
    "golf_ca = pd.concat([golf_la, golf_lb, golf_a])\n",
    "school_ca = pd.concat([schools_la, schools_longbeach, schools_anaheim])\n",
    "\n",
    "houses_ca['dist_hospitals'] = getDist(houses_ca, hospitals_ca)\n",
    "houses_ca['dist_gym'] = getDist(houses_ca, gym_ca)\n",
    "houses_ca['dist_cemetary'] = getDist(houses_ca, cemetary_ca)\n",
    "houses_ca['dist_parks'] = getDist(houses_ca, parks_ca)\n",
    "houses_ca['dist_beaches'] = getDist(houses_ca, beaches_ca)\n",
    "houses_ca['dist_shopping'] = getDist(houses_ca, shopping_ca)\n",
    "houses_ca['dist_grocery'] = getDist(houses_ca, grocery_ca)\n",
    "houses_ca['dist_resturant'] = getDist(houses_ca, resturant_ca)\n",
    "houses_ca['dist_golf'] = getDist(houses_ca, golf_ca)\n",
    "houses_ca['dist_school'] = getDist(houses_ca, school_ca)\n",
    "\n",
    "houses_ca = houses_ca.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bca4d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHICAGO DATA\n",
    "hospitals_il = pd.concat([hospitals_c, hospitals_e, hospitals_n])\n",
    "gym_il = pd.concat([gym_c, gym_e, gym_n])\n",
    "cemetary_il = pd.concat([cemetary_c, cemetary_e, cemetary_n])\n",
    "parks_il = pd.concat([parks_c, parks_e, parks_n])\n",
    "beaches_il = pd.concat([beaches_c, beaches_e, beaches_n])\n",
    "shopping_il = pd.concat([shopping_c, shopping_e, shopping_n])\n",
    "grocery_il = pd.concat([grocery_c, grocery_e, grocery_n])\n",
    "resturant_il = pd.concat([resturant_c, resturant_e, resturant_n])\n",
    "golf_il = pd.concat([golf_c, golf_e, golf_n])\n",
    "school_il = pd.concat([schools_chicago, schools_elgin, schools_naperville])\n",
    "\n",
    "\n",
    "houses_il['dist_hospitals'] = getDist(houses_il, hospitals_il)\n",
    "houses_il['dist_gym'] = getDist(houses_il, gym_il)\n",
    "houses_il['dist_cemetary'] = getDist(houses_il, cemetary_il)\n",
    "houses_il['dist_parks'] = getDist(houses_il, parks_il)\n",
    "houses_il['dist_beaches'] = getDist(houses_il, beaches_il)\n",
    "houses_il['dist_shopping'] = getDist(houses_il, shopping_il)\n",
    "houses_il['dist_grocery'] = getDist(houses_il, grocery_il)\n",
    "houses_il['dist_resturant'] = getDist(houses_il, resturant_il)\n",
    "houses_il['dist_golf'] = getDist(houses_il, golf_il)\n",
    "houses_il['dist_school'] = getDist(houses_il, school_il)\n",
    "\n",
    "houses_il = houses_il.iloc[: , 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfada30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dallas - texas\n",
    "hospitals_tx = pd.concat([hospitals_d, hospitals_fw, hospitals_atx])\n",
    "gym_tx = pd.concat([gym_d, gym_fw, gym_atx])\n",
    "cemetary_tx = pd.concat([cemetary_d, cemetary_fw, cemetary_atx])\n",
    "parks_tx = pd.concat([parks_d, parks_fw, parks_atx])\n",
    "beaches_tx = pd.concat([beaches_d, beaches_fw, beaches_atx])\n",
    "shopping_tx = pd.concat([shopping_d, shopping_fw, shopping_atx])\n",
    "grocery_tx = pd.concat([grocery_d, grocery_fw, grocery_atx])\n",
    "resturant_tx = pd.concat([resturant_d, resturant_fw, resturant_atx])\n",
    "golf_tx = pd.concat([golf_d, golf_fw, golf_atx])\n",
    "school_tx = pd.concat([schools_dallas, schools_fortworth, schools_atx])\n",
    "\n",
    "\n",
    "houses_tx['dist_hospitals'] = getDist(houses_tx, hospitals_tx)\n",
    "houses_tx['dist_gym'] = getDist(houses_tx, gym_tx)\n",
    "houses_tx['dist_cemetary'] = getDist(houses_tx, cemetary_tx)\n",
    "houses_tx['dist_parks'] = getDist(houses_tx, parks_tx)\n",
    "houses_tx['dist_beaches'] = getDist(houses_tx, beaches_tx)\n",
    "houses_tx['dist_shopping'] = getDist(houses_tx, shopping_tx)\n",
    "houses_tx['dist_grocery'] = getDist(houses_tx, grocery_tx)\n",
    "houses_tx['dist_resturant'] = getDist(houses_tx, resturant_tx)\n",
    "houses_tx['dist_golf'] = getDist(houses_tx, golf_tx)\n",
    "houses_tx['dist_school'] = getDist(houses_tx, school_tx)\n",
    "\n",
    "houses_tx = houses_tx.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "414996ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#washington - dc\n",
    "hospitals_dc = pd.concat([hospitals_dc, hospitals_ava, hospitals_alex])\n",
    "gym_dc = pd.concat([gym_dc, gym_ava, gym_alex])\n",
    "cemetary_dc = pd.concat([cemetary_dc, cemetary_ava, cemetary_alex])\n",
    "parks_dc = pd.concat([parks_dc, parks_ava, parks_alex])\n",
    "beaches_dc = pd.concat([beaches_dc, beaches_ava, beaches_alex])\n",
    "shopping_dca = pd.concat([shopping_dc, shopping_ava, shopping_alex])\n",
    "grocery_dc = pd.concat([grocery_dc, grocery_ava, grocery_alex])\n",
    "resturant_dc = pd.concat([resturant_dc, resturant_ava, resturant_alex])\n",
    "golf_dc = pd.concat([golf_dc, golf_ava, golf_alex])\n",
    "school_dc = pd.concat([schools_washington, schools_ava, schools_alex])\n",
    "\n",
    "\n",
    "houses_dc['dist_hospitals'] = getDist(houses_dc, hospitals_dc)\n",
    "houses_dc['dist_gym'] = getDist(houses_dc, gym_dc)\n",
    "houses_dc['dist_cemetary'] = getDist(houses_dc, cemetary_dc)\n",
    "houses_dc['dist_parks'] = getDist(houses_dc, parks_dc)\n",
    "houses_dc['dist_beaches'] = getDist(houses_dc, beaches_dc)\n",
    "houses_dc['dist_shopping'] = getDist(houses_dc, shopping_dc)\n",
    "houses_dc['dist_grocery'] = getDist(houses_dc, grocery_dc)\n",
    "houses_dc['dist_resturant'] = getDist(houses_dc, resturant_dc)\n",
    "houses_dc['dist_golf'] = getDist(houses_dc, golf_dc)\n",
    "houses_dc['dist_school'] = getDist(houses_dc, school_dc)\n",
    "\n",
    "houses_dc = houses_dc.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "76d7bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turned data to csv, put into final data\n",
    "\n",
    "#houses_ca.to_csv(\"mi_final_lametro.csv\")\n",
    "#houses_il.to_csv(\"mi_final_chimetro.csv\")\n",
    "#houses_tx.to_csv(\"mi_final_txmetro.csv\")\n",
    "#houses_dc.to_csv(\"mi_final_dcmetro.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "91c4300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "\n",
    "#transform\n",
    "np.seterr(divide = 'ignore') \n",
    "houses_ca = pd.read_csv('../data/final/metro/mi_final_lametro.csv')\n",
    "houses_ca['log'] = np.log(houses_ca['Norm Price'])\n",
    "houses_ca['sqrt'] = np.sqrt(houses_ca['Norm Price'])\n",
    "houses_ca['cube_root'] = np.power(houses_ca['Norm Price'], 1/3)\n",
    "\n",
    "houses_il = pd.read_csv('../data/final/metro/mi_final_chimetro.csv')\n",
    "houses_il['log'] = np.log(houses_il['Norm Price'])\n",
    "houses_il['sqrt'] = np.sqrt(houses_il['Norm Price'])\n",
    "houses_il['cube_root'] = np.power(houses_il['Norm Price'], 1/3)\n",
    "\n",
    "houses_tx = pd.read_csv('../data/final/metro/mi_final_txmetro.csv')\n",
    "houses_tx['log'] = np.log(houses_tx['Norm Price'])\n",
    "houses_tx['sqrt'] = np.sqrt(houses_tx['Norm Price'])\n",
    "houses_tx['cube_root'] = np.power(houses_tx['Norm Price'], 1/3)\n",
    "\n",
    "houses_dc = pd.read_csv('../data/final/metro/mi_final_dcmetro.csv')\n",
    "houses_dc['log'] = np.log(houses_dc['Norm Price'])\n",
    "houses_dc['sqrt'] = np.sqrt(houses_dc['Norm Price'])\n",
    "houses_dc['cube_root'] = np.power(houses_dc['Norm Price'], 1/3)\n",
    "\n",
    "houses_ca = houses_ca[['Address','City', 'Longitude','Latitude', 'SQ Ft', 'Housing Price','Housing Price/SQ Ft', 'Norm Price', 'log', 'sqrt', 'cube_root', 'dist_hospitals', 'dist_gym', 'dist_cemetary', 'dist_parks', 'dist_beaches', 'dist_shopping', 'dist_grocery', 'dist_resturant', 'dist_golf', 'dist_school']]\n",
    "houses_il = houses_il[['Address','City', 'Longitude','Latitude', 'SQ Ft', 'Housing Price','Housing Price/SQ Ft', 'Norm Price', 'log', 'sqrt', 'cube_root', 'dist_hospitals', 'dist_gym', 'dist_cemetary', 'dist_parks', 'dist_beaches', 'dist_shopping', 'dist_grocery', 'dist_resturant', 'dist_golf', 'dist_school']]\n",
    "houses_tx = houses_tx[['Address','City', 'Longitude','Latitude', 'SQ Ft', 'Housing Price','Housing Price/SQ Ft', 'Norm Price', 'log', 'sqrt', 'cube_root', 'dist_hospitals', 'dist_gym', 'dist_cemetary', 'dist_parks', 'dist_beaches', 'dist_shopping', 'dist_grocery', 'dist_resturant', 'dist_golf', 'dist_school']]\n",
    "houses_dc = houses_dc[['Address','City', 'Longitude','Latitude', 'SQ Ft', 'Housing Price','Housing Price/SQ Ft', 'Norm Price', 'log', 'sqrt', 'cube_root', 'dist_hospitals', 'dist_gym', 'dist_cemetary', 'dist_parks', 'dist_beaches', 'dist_shopping', 'dist_grocery', 'dist_resturant', 'dist_golf', 'dist_school']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "sns.heatmap(houses_ca.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ebbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "sns.heatmap(houses_il.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e9feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "sns.heatmap(houses_tx.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45618f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "sns.heatmap(houses_dc.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "667a716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate to explore\n",
    "houses_ca_eda = houses_ca.iloc[:,19:]\n",
    "houses_il_eda = houses_il.iloc[:,19:]\n",
    "houses_tx_eda = houses_tx.iloc[:,19:]\n",
    "houses_dc_eda = houses_dc.iloc[:,19:]\n",
    "\n",
    "houses_ca_eda = pd.concat([houses_ca['Norm Price'], houses_ca_eda], axis = 1)\n",
    "houses_ca_eda = pd.concat([houses_ca['Housing Price'], houses_ca_eda], axis = 1)\n",
    "\n",
    "houses_il_eda = pd.concat([houses_il['Norm Price'], houses_il_eda], axis = 1)\n",
    "houses_il_eda = pd.concat([houses_il['Housing Price'], houses_il_eda], axis = 1)\n",
    "\n",
    "houses_tx_eda = pd.concat([houses_tx['Norm Price'], houses_tx_eda], axis = 1)\n",
    "houses_tx_eda = pd.concat([houses_tx['Housing Price'], houses_tx_eda], axis = 1)\n",
    "\n",
    "houses_dc_eda = pd.concat([houses_dc['Norm Price'], houses_dc_eda], axis = 1)\n",
    "houses_dc_eda = pd.concat([houses_dc['Housing Price'], houses_dc_eda], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd10853",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(houses_ca_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf7274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(houses_il_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b60b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(houses_tx_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8eaca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(houses_dc_eda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d06237df",
   "metadata": {},
   "source": [
    "### **Regression Analysis**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df81043d",
   "metadata": {},
   "source": [
    "\n",
    "**Data Manipulation** \n",
    "\n",
    "Combining all metro datas into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443227bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining all METRO Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    " \n",
    "lametro = pd.read_csv(\"../data/final/metro/mi_final_lametro.csv\")\n",
    "chimetro = pd.read_csv(\"../data/final/metro/mi_final_chimetro.csv\")\n",
    "dcmetro = pd.read_csv(\"../data/final/metro/mi_final_dcmetro.csv\")\n",
    "dalmetro = pd.read_csv(\"../data/final/metro/mi_final_txmetro.csv\")\n",
    "\n",
    "lametro_adj = lametro[[\"Norm Price\",\"City\",\"dist_hospitals\",\"dist_gym\",\"dist_cemetary\",\"dist_parks\",\"dist_beaches\",\"dist_shopping\",\"dist_grocery\",\"dist_resturant\",\"dist_golf\",\"dist_school\"]]\n",
    "chimetro_adj = chimetro[[\"Norm Price\",\"City\",\"dist_hospitals\",\"dist_gym\",\"dist_cemetary\",\"dist_parks\",\"dist_beaches\",\"dist_shopping\",\"dist_grocery\",\"dist_resturant\",\"dist_golf\",\"dist_school\"]]\n",
    "dcmetro_adj = dcmetro[[\"Norm Price\",\"City\",\"dist_hospitals\",\"dist_gym\",\"dist_cemetary\",\"dist_parks\",\"dist_beaches\",\"dist_shopping\",\"dist_grocery\",\"dist_resturant\",\"dist_golf\",\"dist_school\"]]\n",
    "dalmetro_adj = dalmetro[[\"Norm Price\",\"City\",\"dist_hospitals\",\"dist_gym\",\"dist_cemetary\",\"dist_parks\",\"dist_beaches\",\"dist_shopping\",\"dist_grocery\",\"dist_resturant\",\"dist_golf\",\"dist_school\"]]\n",
    "\n",
    "lametro_adj = lametro_adj.rename(columns={lametro_adj.columns[0]: \"Norm\" })\n",
    "chimetro_adj = chimetro_adj.rename(columns={chimetro_adj.columns[0]: \"Norm\" })\n",
    "dcmetro_adj = dcmetro_adj.rename(columns={dcmetro_adj.columns[0]: \"Norm\" })\n",
    "dalmetro_adj = dalmetro_adj.rename(columns={dalmetro_adj.columns[0]: \"Norm\" })\n",
    "\n",
    "metro = pd.concat([lametro_adj,chimetro_adj,dcmetro_adj,dalmetro_adj])\n",
    "#metro.to_csv(\"metro.csv\")\n",
    "\n",
    "nummetro = metro.drop(columns = [\"City\",\"Norm\"])\n",
    "nummetro = nummetro.reset_index(drop=True)\n",
    "citymetro = metro[[\"City\",\"Norm\"]]\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "nummetro = nummetro[(np.abs(stats.zscore(nummetro)) < 3).all(axis=1)]\n",
    "nummetro = nummetro.join(citymetro)\n",
    "nummetro = nummetro.drop(nummetro.index[nummetro['Norm'] <= 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbc226",
   "metadata": {},
   "source": [
    "**Model I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd026dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "from scipy import stats\n",
    "\n",
    "resulta = sm.OLS(nummetro['Norm'], nummetro.drop(columns=[\"Norm\",\"City\"])).fit() #Initialize the GLS \n",
    "print(resulta.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d8f49",
   "metadata": {},
   "source": [
    "Assumption Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb7672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity: VIF\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "  \n",
    "y, X = dmatrices('Norm ~ dist_hospitals+dist_gym+dist_cemetary+dist_parks+dist_beaches+dist_shopping+dist_grocery+dist_resturant+dist_golf+dist_school', data=nummetro, return_type='dataframe')\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['variable'] = X.columns\n",
    "vif = vif.set_index(\"variable\")\n",
    "vif\n",
    "vif.style.applymap(lambda x: 'background-color : purple' if x>5 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f624ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.api as sms\n",
    "sns.set_style('darkgrid')\n",
    "sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "\n",
    "def linearity_test(model, y):\n",
    "    '''\n",
    "    Function for visually inspecting the assumption of linearity in a linear regression model.\n",
    "    It plots observed vs. predicted values and residuals vs. predicted values.\n",
    "    \n",
    "    Args:\n",
    "    * model - fitted OLS model from statsmodels\n",
    "    * y - observed values\n",
    "    '''\n",
    "    fitted_vals = model.predict()\n",
    "    resids = model.resid\n",
    "\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    \n",
    "    sns.regplot(x=fitted_vals, y=y, lowess=True, ax=ax[0], line_kws={'color': 'red'})\n",
    "    ax[0].set_title('Observed vs. Predicted Values', fontsize=16)\n",
    "    ax[0].set(xlabel='Predicted', ylabel='Observed')\n",
    "\n",
    "    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[1], line_kws={'color': 'red'})\n",
    "    ax[1].set_title('Residuals vs. Predicted Values', fontsize=16)\n",
    "    ax[1].set(xlabel='Predicted', ylabel='Residuals')\n",
    "    \n",
    "linearity_test(resulta, y=nummetro[[\"Norm\"]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379dd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unequal Variances Check: BP\n",
    "import statsmodels.stats.api as sms\n",
    "sms.het_breuschpagan(resulta.resid, resulta.model.exog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality: QQ Plot\n",
    "import statsmodels.stats.api\n",
    "stats.probplot(resulta.resid, dist=\"norm\", plot= plt)\n",
    "plt.title(\"Model1 Residuals Q-Q Plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc5351",
   "metadata": {},
   "source": [
    "**Model II: Reciprocal Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA MANIPULATION\n",
    "import numpy as np\n",
    "nummetro = metro.drop(columns = [\"City\",\"Norm\"])\n",
    "nummetro = np.exp(-nummetro)\n",
    "Q1 = nummetro.quantile(q=.25)\n",
    "Q3 = nummetro.quantile(q=.75)\n",
    "IQR = nummetro.apply(stats.iqr)\n",
    "nummetro = nummetro[~((nummetro < (Q1-1.5*IQR)) | (nummetro > (Q3+1.5*IQR))).any(axis=1)]\n",
    "nummetro = nummetro.reset_index(drop=True)\n",
    "citymetro = metro[[\"City\",\"Norm\"]]\n",
    "citymetro[\"Norm\"] = np.exp(-citymetro[\"Norm\"])\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "\n",
    "nummetro = nummetro.join(citymetro)\n",
    "#nummetro.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "from scipy import stats\n",
    "\n",
    "nummetro = nummetro[nummetro.replace([np.inf, -np.inf], np.nan).notnull().all(axis=1)] \n",
    "resulta = sm.OLS(nummetro['Norm'], nummetro.drop(columns=[\"Norm\",\"City\"])).fit() \n",
    "print(resulta.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unequal Variances: BP\n",
    "import statsmodels.stats.api as sms\n",
    "sms.het_breuschpagan(resulta.resid, resulta.model.exog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4902e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity: VIF\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "  \n",
    "y, X = dmatrices('Norm ~ dist_hospitals+dist_gym+dist_cemetary+dist_parks+dist_beaches+dist_shopping+dist_grocery+dist_resturant+dist_golf+dist_school', data=nummetro, return_type='dataframe')\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['variable'] = X.columns\n",
    "vif = vif.set_index(\"variable\")\n",
    "vif\n",
    "vif.style.applymap(lambda x: 'background-color : purple' if x>5 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.api as sms\n",
    "sns.set_style('darkgrid')\n",
    "sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "\n",
    "linearity_test(resulta, y=nummetro[[\"Norm\"]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1bc83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality: QQ Plot\n",
    "import statsmodels.stats.api\n",
    "stats.probplot(resulta.resid, dist=\"norm\", plot= plt)\n",
    "plt.title(\"Model1 Residuals Q-Q Plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf8c17",
   "metadata": {},
   "source": [
    "**Initial Models: Separate Metro Areas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los Angeles Metro Area\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "from scipy import stats\n",
    "\n",
    "citymetro = lametro_adj[[\"Norm\",\"City\"]]\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "lametro_adj = lametro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "lametro_adj = lametro_adj.reset_index(drop=True)\n",
    "lametro_adj = lametro_adj[(np.abs(stats.zscore(lametro_adj)) < 3).all(axis=1)]\n",
    "lametro_adj = lametro_adj.join(citymetro)\n",
    "resultla = sm.OLS(lametro_adj['Norm'], lametro_adj.drop(columns=[\"Norm\",\"City\"])).fit() #Initialize the GLS \n",
    "print(resultla.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69f1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC Metro Area\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "from scipy import stats\n",
    "\n",
    "citymetro = dcmetro_adj[[\"Norm\",\"City\"]]\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "dcmetro_adj = dcmetro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "dcmetro_adj = dcmetro_adj.reset_index(drop=True)\n",
    "dcmetro_adj = dcmetro_adj[(np.abs(stats.zscore(dcmetro_adj)) < 3).all(axis=1)]\n",
    "dcmetro_adj = dcmetro_adj.join(citymetro)\n",
    "resultdc = sm.OLS(dcmetro_adj['Norm'], dcmetro_adj.drop(columns=[\"Norm\",\"City\"])).fit() #Initialize the GLS \n",
    "print(resultdc.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f95cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dallas Metro Area\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "from scipy import stats\n",
    "\n",
    "citymetro = dalmetro_adj[[\"Norm\",\"City\"]]\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "dalmetro_adj = dalmetro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "dalmetro_adj = dalmetro_adj.reset_index(drop=True)\n",
    "dalmetro_adj = dalmetro_adj[(np.abs(stats.zscore(dalmetro_adj)) < 3).all(axis=1)]\n",
    "dalmetro_adj = dalmetro_adj.join(citymetro)\n",
    "resultdal = sm.OLS(dalmetro_adj['Norm'], dalmetro_adj.drop(columns=[\"Norm\",\"City\"])).fit() #Initialize the GLS \n",
    "print(resultdal.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chicago Metro Area\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "from scipy import stats\n",
    "\n",
    "citymetro = chimetro_adj[[\"Norm\",\"City\"]]\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "chimetro_adj = chimetro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "chimetro_adj = chimetro_adj.reset_index(drop=True)\n",
    "chimetro_adj = chimetro_adj[(np.abs(stats.zscore(chimetro_adj)) < 3).all(axis=1)]\n",
    "chimetro_adj = chimetro_adj.join(citymetro)\n",
    "resultchi = sm.OLS(chimetro_adj['Norm'], chimetro_adj.drop(columns=[\"Norm\",\"City\"])).fit() #Initialize the GLS \n",
    "print(resultchi.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ebf9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF Check for All Four Metro Areas\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "  \n",
    "yla, Xla = dmatrices('Norm ~ dist_hospitals+dist_gym+dist_cemetary+dist_parks+dist_beaches+dist_shopping+dist_grocery+dist_resturant+dist_golf+dist_school', data=lametro_adj, return_type='dataframe')\n",
    "ydc, Xdc = dmatrices('Norm ~ dist_hospitals+dist_gym+dist_cemetary+dist_parks+dist_beaches+dist_shopping+dist_grocery+dist_resturant+dist_golf+dist_school', data=dcmetro_adj, return_type='dataframe')\n",
    "ydal, Xdal = dmatrices('Norm ~ dist_hospitals+dist_gym+dist_cemetary+dist_parks+dist_beaches+dist_shopping+dist_grocery+dist_resturant+dist_golf+dist_school', data=dalmetro_adj, return_type='dataframe')\n",
    "ychi, Xchi = dmatrices('Norm ~ dist_hospitals+dist_gym+dist_cemetary+dist_parks+dist_beaches+dist_shopping+dist_grocery+dist_resturant+dist_golf+dist_school', data=chimetro_adj, return_type='dataframe')\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF LA'] = [variance_inflation_factor(Xla.values, i) for i in range(Xla.shape[1])]\n",
    "vif['VIF DC'] = [variance_inflation_factor(Xdc.values, i) for i in range(Xdc.shape[1])]\n",
    "vif['VIF Dal'] = [variance_inflation_factor(Xdal.values, i) for i in range(Xdal.shape[1])]\n",
    "vif['VIF CHI'] = [variance_inflation_factor(Xchi.values, i) for i in range(Xchi.shape[1])]\n",
    "vif['variable'] = Xla.columns\n",
    "vif = vif.set_index(\"variable\")\n",
    "vif\n",
    "vif.style.applymap(lambda x: 'background-color : purple' if x>5 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482dc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNEQUAL VARIANCE for all Metro Areas\n",
    "import statsmodels.stats.api as sms\n",
    "la = sms.het_breuschpagan(resultla.resid, resultla.model.exog)\n",
    "dc = sms.het_breuschpagan(resultdc.resid, resultdc.model.exog)\n",
    "dal = sms.het_breuschpagan(resultdal.resid, resultdal.model.exog)\n",
    "chi = sms.het_breuschpagan(resultchi.resid, resultchi.model.exog)\n",
    "print(la,dc,dal,chi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7059a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALITY for all Metro Areas\n",
    "import statsmodels.stats.api\n",
    "la = statsmodels.stats.stattools.jarque_bera(resultla.resid)\n",
    "dc = statsmodels.stats.stattools.jarque_bera(resultdc.resid)\n",
    "dal = statsmodels.stats.stattools.jarque_bera(resultdal.resid)\n",
    "chi = statsmodels.stats.stattools.jarque_bera(resultchi.resid)\n",
    "print(la,dc,dal,chi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360be73d",
   "metadata": {},
   "source": [
    "**Model II for separate metros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e91a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los Angeles\n",
    "import numpy as np\n",
    "nummetro = lametro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "nummetro = np.exp(-nummetro)\n",
    "Q1 = nummetro.quantile(q=.25)\n",
    "Q3 = nummetro.quantile(q=.75)\n",
    "IQR = nummetro.apply(stats.iqr)\n",
    "nummetro = nummetro[~((nummetro < (Q1-1.5*IQR)) | (nummetro > (Q3+1.5*IQR))).any(axis=1)]\n",
    "nummetro = nummetro.reset_index(drop=True)\n",
    "citymetro = lametro_adj[[\"City\",\"Norm\"]]\n",
    "citymetro[\"Norm\"] = np.exp(-citymetro[\"Norm\"])\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "nummetro = nummetro.join(citymetro)\n",
    "resultla = sm.OLS(nummetro['Norm'], nummetro.drop(columns=[\"Norm\",\"City\"])).fit() #Initialize the GLS \n",
    "print(resultla.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC\n",
    "import numpy as np\n",
    "nummetro = dcmetro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "nummetro = np.exp(-nummetro)\n",
    "Q1 = nummetro.quantile(q=.25)\n",
    "Q3 = nummetro.quantile(q=.75)\n",
    "IQR = nummetro.apply(stats.iqr)\n",
    "nummetro = nummetro[~((nummetro < (Q1-1.5*IQR)) | (nummetro > (Q3+1.5*IQR))).any(axis=1)]\n",
    "nummetro = nummetro.reset_index(drop=True)\n",
    "citymetro = dcmetro_adj[[\"City\",\"Norm\"]]\n",
    "citymetro[\"Norm\"] = np.exp(-citymetro[\"Norm\"])\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "nummetro = nummetro.join(citymetro)\n",
    "resultdc = sm.OLS(nummetro['Norm'], nummetro.drop(columns=[\"Norm\",\"City\",\"dist_hospitals\",\"dist_school\"])).fit() #Initialize the GLS \n",
    "print(resultdc.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7826f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chicago\n",
    "import numpy as np\n",
    "nummetro = chimetro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "nummetro = np.exp(-nummetro)\n",
    "Q1 = nummetro.quantile(q=.25)\n",
    "Q3 = nummetro.quantile(q=.75)\n",
    "IQR = nummetro.apply(stats.iqr)\n",
    "nummetro = nummetro[~((nummetro < (Q1-1.5*IQR)) | (nummetro > (Q3+1.5*IQR))).any(axis=1)]\n",
    "nummetro = nummetro.reset_index(drop=True)\n",
    "citymetro = chimetro_adj[[\"City\",\"Norm\"]]\n",
    "citymetro[\"Norm\"] = np.exp(-citymetro[\"Norm\"])\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "nummetro = nummetro.join(citymetro)\n",
    "resultchi = sm.OLS(nummetro['Norm'], nummetro.drop(columns=[\"Norm\",\"City\",\"dist_cemetary\",\"dist_beaches\",\"dist_shopping\",\"dist_grocery\"])).fit() #Initialize the GLS \n",
    "print(resultchi.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dallas\n",
    "import numpy as np\n",
    "nummetro = dalmetro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "#nummetro = np.exp(-nummetro)\n",
    "Q1 = nummetro.quantile(q=.25)\n",
    "Q3 = nummetro.quantile(q=.75)\n",
    "IQR = nummetro.apply(stats.iqr)\n",
    "nummetro = nummetro[~((nummetro < (Q1-1.5*IQR)) | (nummetro > (Q3+1.5*IQR))).any(axis=1)]\n",
    "nummetro = nummetro.reset_index(drop=True)\n",
    "citymetro = dalmetro_adj[[\"City\",\"Norm\"]]\n",
    "citymetro[\"Norm\"] = np.exp(-citymetro[\"Norm\"])\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "nummetro = nummetro.join(citymetro)\n",
    "resultdal = sm.OLS(nummetro['Norm'], nummetro.drop(columns=[\"Norm\",\"City\"])).fit() #Initialize the GLS \n",
    "print(resultdal.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc184b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality Check for LA & Dallas (not shown on summary)\n",
    "import statsmodels.stats.api\n",
    "la = statsmodels.stats.stattools.jarque_bera(resultla.resid)\n",
    "dc = statsmodels.stats.stattools.jarque_bera(resultdc.resid)\n",
    "chi = statsmodels.stats.stattools.jarque_bera(resultchi.resid)\n",
    "dal = statsmodels.stats.stattools.jarque_bera(resultdal.resid)\n",
    "print(la,dc,chi,dal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNEQUAL VARIANCE for all Metro Areas\n",
    "import statsmodels.stats.api as sms\n",
    "la = sms.het_breuschpagan(resultla.resid, resultla.model.exog)\n",
    "dc = sms.het_breuschpagan(resultdc.resid, resultdc.model.exog)\n",
    "dal = sms.het_breuschpagan(resultdal.resid, resultdal.model.exog)\n",
    "chi = sms.het_breuschpagan(resultchi.resid, resultchi.model.exog)\n",
    "print(la,dc,dal,chi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a92cc5bf",
   "metadata": {},
   "source": [
    "### Additional Codes (Interactive Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98103c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "lametro = pd.read_csv(\"../data/final/final_lametro.csv\")\n",
    "dcmetro = pd.read_csv(\"../data/final/final_dcmetro.csv\")\n",
    "chimetro = pd.read_csv(\"../data/final/final_chimetro.csv\")\n",
    "dalmetro = pd.read_csv(\"../data/final/final_txmetro.csv\")\n",
    "\n",
    "\n",
    "lametro_adj = lametro[[\"Norm Price\", \"City\", \"Longitude\", \"Latitude\"]]\n",
    "lametro_adj[\"Type\"] = \"House\"\n",
    "dcmetro_adj = dcmetro[[\"Norm Price\", \"City\", \"Longitude\", \"Latitude\"]]\n",
    "dcmetro_adj[\"Type\"] = \"House\"\n",
    "chimetro_adj = chimetro[[\"Norm Price\", \"City\", \"Longitude\", \"Latitude\"]]\n",
    "chimetro_adj[\"Type\"] = \"House\"\n",
    "dalmetro_adj = dalmetro[[\"Norm Price\", \"City\", \"Longitude\", \"Latitude\"]]\n",
    "dalmetro_adj[\"Type\"] = \"House\"\n",
    "\n",
    "\n",
    "lametro_adj = lametro_adj.rename(columns={lametro_adj.columns[0]: \"Norm\"})\n",
    "dcmetro_adj = dcmetro_adj.rename(columns={dcmetro_adj.columns[0]: \"Norm\"})\n",
    "chimetro_adj = chimetro_adj.rename(columns={dcmetro_adj.columns[0]: \"Norm\"})\n",
    "dalmetro_adj = dalmetro_adj.rename(columns={dcmetro_adj.columns[0]: \"Norm\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546bdbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "houses = pd.read_csv('../data/final/houses_clean.csv')\n",
    "\n",
    "houses_la = houses[houses['City'] == 'Los Angeles']\n",
    "houses_anaheim = houses[houses['City'] == 'Anaheim']\n",
    "houses_longbeach = houses[houses['City'] == 'Long Beach']\n",
    "\n",
    "houses_dallas = houses[houses['City'] == 'Dallas']\n",
    "houses_fortworth = houses[houses['City'] == 'Fort Worth']\n",
    "houses_arlington_tx = houses[houses['City'] == 'ArlingtonTX']\n",
    "\n",
    "houses_chicago = houses[houses['City'] == 'Chicago']\n",
    "houses_naperville = houses[houses['City'] == 'Naperville']\n",
    "houses_elgin = houses[houses['City'] == 'Elgin']\n",
    "\n",
    "houses_washington = houses[houses['City'] == 'Washington']\n",
    "houses_alexandria = houses[houses['City'] == 'Alexandria']\n",
    "houses_arlington_va = houses[houses['City'] == 'ArlingtonVA']\n",
    "\n",
    "\n",
    "schools = pd.read_csv('../data/final/schools.csv')\n",
    "schools_la = schools[schools['City'] == 'Los Angeles']\n",
    "schools_anaheim = schools[schools['City'] == 'Anaheim']\n",
    "schools_longbeach = schools[schools['City'] == 'Long Beach']\n",
    "\n",
    "schools_dallas = schools[schools['City'] == 'Dallas']\n",
    "schools_fortworth = schools[schools['City'] == 'Fort Worth']\n",
    "schools_atx = schools[schools['City'] == 'ArlingtonTX']\n",
    "\n",
    "schools_chicago = schools[schools['City'] == 'Chicago']\n",
    "schools_naperville = schools[schools['City'] == 'Naperville']\n",
    "schools_elgin = schools[schools['City'] == 'Elgin']\n",
    "\n",
    "schools_washington = schools[schools['City'] == 'Washington']\n",
    "schools_alex = schools[schools['City'] == 'Alexandria']\n",
    "schools_ava = schools[schools['City'] == 'ArlingtonVA']\n",
    "\n",
    "\n",
    "hospitals = pd.read_csv('../data/final/hospital.csv')\n",
    "\n",
    "hospitals_la = hospitals[hospitals['City'] == 'Los Angeles']\n",
    "hospitals_a = hospitals[hospitals['City'] == 'Anaheim']\n",
    "hospitals_lb = hospitals[hospitals['City'] == 'Long Beach']\n",
    "\n",
    "hospitals_d = hospitals[hospitals['City'] == 'Dallas']\n",
    "hospitals_fw = hospitals[hospitals['City'] == 'Fort Worth']\n",
    "hospitals_atx = hospitals[hospitals['City'] == 'ArlingtonTX']\n",
    "\n",
    "hospitals_c = hospitals[hospitals['City'] == 'Chicago']\n",
    "hospitals_n = hospitals[hospitals['City'] == 'Naperville']\n",
    "hospitals_e = hospitals[hospitals['City'] == 'Elgin']\n",
    "\n",
    "hospitals_dc = schools[schools['City'] == 'Washington']\n",
    "hospitals_alex = schools[schools['City'] == 'Alexandria']\n",
    "hospitals_ava = schools[schools['City'] == 'ArlingtonVA']\n",
    "\n",
    "grocery = pd.read_csv('../data/final/grocery.csv')\n",
    "\n",
    "grocery_la = grocery[grocery['City'] == 'Los Angeles']\n",
    "grocery_a = grocery[grocery['City'] == 'Anaheim']\n",
    "grocery_lb = grocery[grocery['City'] == 'Long Beach']\n",
    "\n",
    "grocery_d = grocery[grocery['City'] == 'Dallas']\n",
    "grocery_fw = grocery[grocery['City'] == 'Fort Worth']\n",
    "grocery_atx = grocery[grocery['City'] == 'ArlingtonTX']\n",
    "\n",
    "grocery_c = grocery[grocery['City'] == 'Chicago']\n",
    "grocery_n = grocery[grocery['City'] == 'Naperville']\n",
    "grocery_e = grocery[grocery['City'] == 'Elgin']\n",
    "\n",
    "grocery_dc = grocery[grocery['City'] == 'Washington']\n",
    "grocery_alex = grocery[grocery['City'] == 'Alexandria']\n",
    "grocery_ava = grocery[grocery['City'] == 'ArlingtonVA']\n",
    "\n",
    "gym = pd.read_csv('../data/final/gym.csv')\n",
    "\n",
    "gym_la = gym[gym['City'] == 'Los Angeles']\n",
    "gym_lb = gym[gym['City'] == 'Long Beach']\n",
    "gym_a = gym[gym['City'] == 'Anaheim']\n",
    "\n",
    "gym_d = gym[gym['City'] == 'Dallas']\n",
    "gym_atx = gym[gym['City'] == 'ArlingtonTX']\n",
    "gym_fw = gym[gym['City'] == 'Fort Worth']\n",
    "\n",
    "gym_c = gym[gym['City'] == 'Chicago']\n",
    "gym_e = gym[gym['City'] == 'Elgin']\n",
    "gym_n = gym[gym['City'] == 'Naperville']\n",
    "\n",
    "gym_dc = gym[gym['City'] == 'Washington']\n",
    "gym_ava = gym[gym['City'] == 'ArlingtonVA']\n",
    "gym_alex = gym[gym['City'] == 'Alexandria']\n",
    "\n",
    "parks = pd.read_csv('../data/final/parks.csv')\n",
    "\n",
    "parks_la = parks[parks['City'] == 'Los Angeles']\n",
    "parks_lb = parks[parks['City'] == 'Long Beach']\n",
    "parks_a = parks[parks['City'] == 'Anaheim']\n",
    "\n",
    "parks_d = parks[parks['City'] == 'Dallas']\n",
    "parks_atx = parks[parks['City'] == 'ArlingtonTX']\n",
    "parks_fw = parks[parks['City'] == 'Fort Worth']\n",
    "\n",
    "parks_c = parks[parks['City'] == 'Chicago']\n",
    "parks_e = parks[parks['City'] == 'Elgin']\n",
    "parks_n = parks[parks['City'] == 'Naperville']\n",
    "\n",
    "parks_dc = parks[parks['City'] == 'Washington']\n",
    "parks_ava = parks[parks['City'] == 'ArlingtonVA']\n",
    "parks_alex = parks[parks['City'] == 'Alexandria']\n",
    "\n",
    "beaches = pd.read_csv('../data/final/beach.csv')\n",
    "\n",
    "beaches_la = beaches[beaches['City'] == 'Los Angeles']\n",
    "beaches_lb = beaches[beaches['City'] == 'Long Beach']\n",
    "beaches_a = beaches[beaches['City'] == 'Anaheim']\n",
    "\n",
    "beaches_d = beaches[beaches['City'] == 'Dallas']\n",
    "beaches_atx = beaches[beaches['City'] == 'ArlingtonTX']\n",
    "beaches_fw = beaches[beaches['City'] == 'Fort Worth']\n",
    "\n",
    "beaches_c = beaches[beaches['City'] == 'Chicago']\n",
    "beaches_e = beaches[beaches['City'] == 'Elgin']\n",
    "beaches_n = beaches[beaches['City'] == 'Naperville']\n",
    "\n",
    "beaches_dc = beaches[beaches['City'] == 'Washington']\n",
    "beaches_ava = beaches[beaches['City'] == 'ArlingtonVA']\n",
    "beaches_alex = beaches[beaches['City'] == 'Alexandria']\n",
    "\n",
    "cemetary = pd.read_csv('../data/final/cemetary.csv')\n",
    "\n",
    "cemetary_la = cemetary[cemetary['City'] == 'Los Angeles']\n",
    "cemetary_lb = cemetary[cemetary['City'] == 'Long Beach']\n",
    "cemetary_a = cemetary[cemetary['City'] == 'Anaheim']\n",
    "\n",
    "cemetary_d = cemetary[cemetary['City'] == 'Dallas']\n",
    "cemetary_atx = cemetary[cemetary['City'] == 'ArlingtonTX']\n",
    "cemetary_fw = cemetary[cemetary['City'] == 'Fort Worth']\n",
    "\n",
    "cemetary_c = cemetary[cemetary['City'] == 'Chicago']\n",
    "cemetary_e = cemetary[cemetary['City'] == 'Elgin']\n",
    "cemetary_n = cemetary[cemetary['City'] == 'Naperville']\n",
    "\n",
    "cemetary_dc = cemetary[cemetary['City'] == 'Washington']\n",
    "cemetary_ava = cemetary[cemetary['City'] == 'ArlingtonVA']\n",
    "cemetary_alex = cemetary[cemetary['City'] == 'Alexandria']\n",
    "\n",
    "shopping = pd.read_csv('../data/final/shopping.csv')\n",
    "\n",
    "shopping_la = shopping[shopping['City'] == 'Los Angeles']\n",
    "shopping_lb = shopping[shopping['City'] == 'Long Beach']\n",
    "shopping_a = shopping[shopping['City'] == 'Anaheim']\n",
    "\n",
    "shopping_d = shopping[shopping['City'] == 'Dallas']\n",
    "shopping_atx = shopping[shopping['City'] == 'ArlingtonTX']\n",
    "shopping_fw = shopping[shopping['City'] == 'Fort Worth']\n",
    "\n",
    "shopping_c = shopping[shopping['City'] == 'Chicago']\n",
    "shopping_e = shopping[shopping['City'] == 'Elgin']\n",
    "shopping_n = shopping[shopping['City'] == 'Naperville']\n",
    "\n",
    "shopping_dc = shopping[shopping['City'] == 'Washington']\n",
    "shopping_ava = shopping[shopping['City'] == 'ArlingtonVA']\n",
    "shopping_alex = shopping[shopping['City'] == 'Alexandria']\n",
    "\n",
    "resturant = pd.read_csv('../data/final/restaurants.csv')\n",
    "\n",
    "resturant_la = resturant[resturant['City'] == 'Los Angeles']\n",
    "resturant_lb = resturant[resturant['City'] == 'Long Beach']\n",
    "resturant_a = resturant[resturant['City'] == 'Anaheim']\n",
    "\n",
    "resturant_d = resturant[resturant['City'] == 'Dallas']\n",
    "resturant_atx = resturant[resturant['City'] == 'ArlingtonTX']\n",
    "resturant_fw = resturant[resturant['City'] == 'Fort Worth']\n",
    "\n",
    "resturant_c = resturant[resturant['City'] == 'Chicago']\n",
    "resturant_e = resturant[resturant['City'] == 'Elgin']\n",
    "resturant_n = resturant[resturant['City'] == 'Naperville']\n",
    "\n",
    "resturant_dc = resturant[resturant['City'] == 'Washington']\n",
    "resturant_ava = resturant[resturant['City'] == 'ArlingtonVA']\n",
    "resturant_alex = resturant[resturant['City'] == 'Alexandria']\n",
    "\n",
    "golf = pd.read_csv('../data/final/golf.csv')\n",
    "\n",
    "golf_la = golf[golf['City'] == 'Los Angeles']\n",
    "golf_lb = golf[golf['City'] == 'Long Beach']\n",
    "golf_a = golf[golf['City'] == 'Anaheim']\n",
    "\n",
    "golf_d = golf[golf['City'] == 'Dallas']\n",
    "golf_atx = golf[golf['City'] == 'ArlingtonTX']\n",
    "golf_fw = golf[golf['City'] == 'Fort Worth']\n",
    "\n",
    "golf_c = golf[golf['City'] == 'Chicago']\n",
    "golf_e = golf[golf['City'] == 'Elgin']\n",
    "golf_n = golf[golf['City'] == 'Naperville']\n",
    "\n",
    "golf_dc = golf[golf['City'] == 'Washington']\n",
    "golf_ava = golf[golf['City'] == 'ArlingtonVA']\n",
    "golf_alex = golf[golf['City'] == 'Alexandria']\n",
    "\n",
    "hospitals_ca = pd.concat([hospitals_la, hospitals_lb, hospitals_a])\n",
    "hospitals_ca[\"Type\"] = \"Hospitals\"\n",
    "hospitals_ca = hospitals_ca.dropna()\n",
    "gym_ca = pd.concat([gym_la, gym_lb, gym_a])\n",
    "gym_ca[\"Type\"] = \"Gym\"\n",
    "gym_ca = gym_ca.dropna()\n",
    "cemetary_ca = pd.concat([cemetary_la, cemetary_lb, cemetary_a])\n",
    "cemetary_ca[\"Type\"] = \"Cemetary\"\n",
    "cemetary_ca = cemetary_ca.dropna()\n",
    "\n",
    "parks_ca = pd.concat([parks_la, parks_lb, parks_la])\n",
    "parks_ca[\"Type\"] = \"Parks\"\n",
    "parks_ca = parks_ca.dropna()\n",
    "\n",
    "beaches_ca = pd.concat([beaches_la, beaches_lb, beaches_a])\n",
    "beaches_ca[\"Type\"] = \"Beaches\"\n",
    "beaches_ca = beaches_ca.dropna()\n",
    "\n",
    "shopping_ca = pd.concat([shopping_la, shopping_lb, shopping_a])\n",
    "shopping_ca[\"Type\"] = \"Shopping\"\n",
    "shopping_ca = shopping_ca.dropna()\n",
    "\n",
    "grocery_ca = pd.concat([grocery_la, grocery_lb, grocery_a])\n",
    "grocery_ca[\"Type\"] = \"Grocery\"\n",
    "grocery_ca = grocery_ca.dropna()\n",
    "\n",
    "resturant_ca = pd.concat([resturant_la, resturant_lb, resturant_a])\n",
    "resturant_ca[\"Type\"] = \"Restaurants\"\n",
    "resturant_ca = resturant_ca.dropna()\n",
    "\n",
    "golf_ca = pd.concat([golf_la, golf_lb, golf_a])\n",
    "golf_ca[\"Type\"] = \"Golf\"\n",
    "golf_ca = golf_ca.dropna()\n",
    "\n",
    "school_ca = pd.concat([schools_la, schools_longbeach, schools_anaheim])\n",
    "school_ca[\"Type\"] = \"School\"\n",
    "school_ca = school_ca.dropna()\n",
    "\n",
    "\n",
    "hospitals_dc = pd.concat([hospitals_dc, hospitals_ava, hospitals_alex])\n",
    "hospitals_dc[\"Type\"] = \"Hospitals\"\n",
    "hospitals_dc = hospitals_dc.dropna()\n",
    "gym_dc = pd.concat([gym_dc, gym_ava, gym_alex])\n",
    "gym_dc[\"Type\"] = \"Gym\"\n",
    "gym_dc = gym_dc.dropna()\n",
    "cemetary_dc = pd.concat([cemetary_dc, cemetary_ava, cemetary_alex])\n",
    "cemetary_dc[\"Type\"] = \"Cemetary\"\n",
    "cemetary_dc = cemetary_dc.dropna()\n",
    "parks_dc = pd.concat([parks_dc, parks_ava, parks_alex])\n",
    "parks_dc[\"Type\"] = \"Parks\"\n",
    "parks_dc = parks_dc.dropna()\n",
    "beaches_dc = pd.concat([beaches_dc, beaches_ava, beaches_alex])\n",
    "beaches_dc[\"Type\"] = \"Beaches\"\n",
    "beaches_dc = beaches_dc.dropna()\n",
    "shopping_dc= pd.concat([shopping_dc, shopping_ava, shopping_alex])\n",
    "shopping_dc[\"Type\"] = \"Shopping\"\n",
    "shopping_dc = shopping_dc.dropna()\n",
    "grocery_dc = pd.concat([grocery_dc, grocery_ava, grocery_alex])\n",
    "grocery_dc[\"Type\"] = \"Grocery\"\n",
    "grocery_dc = grocery_dc.dropna()\n",
    "resturant_dc = pd.concat([resturant_dc, resturant_ava, resturant_alex])\n",
    "resturant_dc[\"Type\"] = \"Restaurants\"\n",
    "resturant_dc = resturant_dc.dropna()\n",
    "golf_dc = pd.concat([golf_dc, golf_ava, golf_alex])\n",
    "golf_dc[\"Type\"] = \"Golf\"\n",
    "golf_dc = golf_dc.dropna()\n",
    "school_dc = pd.concat([schools_washington, schools_ava, schools_alex])\n",
    "school_dc[\"Type\"] = \"School\"\n",
    "school_dc = school_dc.dropna()\n",
    "\n",
    "hospitals_chi = pd.concat([hospitals_c, hospitals_e, hospitals_n])\n",
    "hospitals_chi[\"Type\"] = \"Hospitals\"\n",
    "hospitals_chi = hospitals_chi.dropna()\n",
    "gym_chi = pd.concat([gym_c, gym_e, gym_n])\n",
    "gym_chi[\"Type\"] = \"Gym\"\n",
    "gym_chi = gym_chi.dropna()\n",
    "cemetary_chi = pd.concat([cemetary_c, cemetary_e, cemetary_n])\n",
    "cemetary_chi[\"Type\"] = \"Cemetary\"\n",
    "cemetary_chi = cemetary_chi.dropna()\n",
    "parks_chi = pd.concat([parks_c, parks_e, parks_n])\n",
    "parks_chi[\"Type\"] = \"Parks\"\n",
    "parks_chi = parks_chi.dropna()\n",
    "beaches_chi = pd.concat([beaches_c, beaches_e, beaches_n])\n",
    "beaches_chi[\"Type\"] = \"Beaches\"\n",
    "beaches_chi = beaches_chi.dropna()\n",
    "shopping_chi= pd.concat([shopping_c, shopping_e, shopping_n])\n",
    "shopping_chi[\"Type\"] = \"Shopping\"\n",
    "shopping_chi = shopping_chi.dropna()\n",
    "grocery_chi = pd.concat([grocery_c, grocery_e, grocery_n])\n",
    "grocery_chi[\"Type\"] = \"Grocery\"\n",
    "grocery_chi = grocery_dc.dropna()\n",
    "resturant_chi = pd.concat([resturant_c, resturant_e, resturant_n])\n",
    "resturant_chi[\"Type\"] = \"Restaurants\"\n",
    "resturant_chi = resturant_chi.dropna()\n",
    "golf_chi = pd.concat([golf_c, golf_e, golf_n])\n",
    "golf_chi[\"Type\"] = \"Golf\"\n",
    "golf_chi = golf_chi.dropna()\n",
    "school_chi= pd.concat([schools_chicago, schools_naperville, schools_elgin])\n",
    "school_chi[\"Type\"] = \"School\"\n",
    "school_chi = school_chi.dropna()\n",
    "\n",
    "hospitals_dal = pd.concat([hospitals_d, hospitals_atx, hospitals_fw])\n",
    "hospitals_dal[\"Type\"] = \"Hospitals\"\n",
    "hospitals_dal = hospitals_dal.dropna()\n",
    "gym_dal = pd.concat([gym_d, gym_atx, gym_fw])\n",
    "gym_dal[\"Type\"] = \"Gym\"\n",
    "gym_dal = gym_dal.dropna()\n",
    "cemetary_dal = pd.concat([cemetary_d, cemetary_atx, cemetary_fw])\n",
    "cemetary_dal[\"Type\"] = \"Cemetary\"\n",
    "cemetary_dal = cemetary_dal.dropna()\n",
    "parks_dal = pd.concat([parks_d, parks_atx, parks_fw])\n",
    "parks_dal[\"Type\"] = \"Parks\"\n",
    "parks_dal = parks_dal.dropna()\n",
    "beaches_dal = pd.concat([beaches_d, beaches_atx, beaches_fw])\n",
    "beaches_dal[\"Type\"] = \"Beaches\"\n",
    "beaches_dal = beaches_dal.dropna()\n",
    "shopping_dal= pd.concat([shopping_d, shopping_atx, shopping_fw])\n",
    "shopping_dal[\"Type\"] = \"Shopping\"\n",
    "shopping_dal = shopping_dal.dropna()\n",
    "grocery_dal = pd.concat([grocery_d, grocery_atx, grocery_fw])\n",
    "grocery_dal[\"Type\"] = \"Grocery\"\n",
    "grocery_dal = grocery_dal.dropna()\n",
    "resturant_dal = pd.concat([resturant_d, resturant_atx, resturant_fw])\n",
    "resturant_dal[\"Type\"] = \"Restaurants\"\n",
    "resturant_dal = resturant_chi.dropna()\n",
    "golf_dal = pd.concat([golf_d, golf_atx, golf_fw])\n",
    "golf_dal[\"Type\"] = \"Golf\"\n",
    "golf_dal = golf_dal.dropna()\n",
    "school_dal= pd.concat([schools_dallas, schools_atx, schools_fortworth])\n",
    "school_dal[\"Type\"] = \"School\"\n",
    "school_dal = school_dal.dropna()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a049294",
   "metadata": {},
   "source": [
    "### **MAP**: A visualization of the neighborhood and its houses.\n",
    "\n",
    "Through the python package *folium*, here is an interactive maps of the locations of the housing and neighborhood landmarks of the Metro Areas.\n",
    "\n",
    "**LOS ANGELES METRO AREA**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fba29d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "m = folium.Map(location = [34.052235, -118.243683], zoom_start = 11)\n",
    "\n",
    "cols = [\"Type\", \"Longitude\", \"Latitude\"]\n",
    "for name, lat, lng in lametro_adj[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"red\", radius = 10, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in hospitals_ca[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    blue = folium.Circle([float(lat), float(lng)], color = \"blue\", radius = 5, popup = popup)\n",
    "    m.add_child(blue)\n",
    "for name, lng, lat in gym_ca[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"yellow\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in cemetary_ca[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"green\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in parks_ca[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"purple\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in beaches_ca[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"black\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in shopping_ca[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"orange\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in grocery_ca[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"lightblue\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in resturant_ca[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"gray\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in golf_ca[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"cadetblue\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in school_ca[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"beige\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "    \n",
    "fig = folium.Figure(width = 900, height = 400)\n",
    "fig.add_child(m)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb116fda",
   "metadata": {},
   "source": [
    "**DC METRO AREA**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309f6e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "\n",
    "m = folium.Map(location = [38.900497,  -77.007507], zoom_start = 11)\n",
    "\n",
    "cols = [\"Type\", \"Longitude\", \"Latitude\"]\n",
    "for name, lat, lng in dcmetro_adj[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"red\", radius = 10, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in hospitals_dc[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"blue\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in gym_dc[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"yellow\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in cemetary_dc[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"green\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in parks_dc[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"purple\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in beaches_dc[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"black\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in shopping_dc[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"orange\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in grocery_dc[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"lightblue\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in resturant_dc[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"gray\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in golf_dc[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"cadetblue\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in school_dc[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"beige\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "    \n",
    "fig = folium.Figure(width = 900, height = 400)\n",
    "fig.add_child(m)\n",
    "m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "912c9377",
   "metadata": {},
   "source": [
    "**CHICAGO METRO AREA**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "\n",
    "m = folium.Map(location = [41.8781,  -87.6298], zoom_start = 11)\n",
    "\n",
    "cols = [\"Type\", \"Longitude\", \"Latitude\"]\n",
    "for name, lat, lng in chimetro_adj[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"red\", radius = 10, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in hospitals_chi[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"blue\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in gym_chi[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"yellow\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in cemetary_chi[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"green\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in parks_chi[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"purple\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in beaches_chi[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"black\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in shopping_chi[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"orange\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in grocery_chi[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"lightblue\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in resturant_chi[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"gray\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in golf_chi[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"cadetblue\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in school_chi[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"beige\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "    \n",
    "fig = folium.Figure(width = 900, height = 400)\n",
    "fig.add_child(m)\n",
    "m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ad4e181",
   "metadata": {},
   "source": [
    "**DALLAS METRO AREA**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81241cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "\n",
    "m = folium.Map(location = [32.7767,  -96.7970], zoom_start = 11)\n",
    "\n",
    "cols = [\"Type\", \"Longitude\", \"Latitude\"]\n",
    "for name, lat, lng in dalmetro_adj[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"red\", radius = 10, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in hospitals_dal[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"blue\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in gym_dal[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"yellow\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in cemetary_dal[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"green\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in parks_dal[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"purple\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in beaches_dal[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"black\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in shopping_dal[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"orange\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in grocery_dal[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"lightblue\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in resturant_dal[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"gray\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in golf_dal[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"cadetblue\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "for name, lng, lat in school_dal[cols].itertuples(index = False):\n",
    "    popup = folium.Popup(name, parse_html = True)\n",
    "    circle = folium.Circle([float(lat), float(lng)], color = \"beige\", radius = 5, popup = popup)\n",
    "    m.add_child(circle)\n",
    "    \n",
    "fig = folium.Figure(width = 900, height = 400)\n",
    "fig.add_child(m)\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e4a38a8137c1a4cb4c7901223266b1dfbc056b248676f3e6576fa0f54bd3eb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
